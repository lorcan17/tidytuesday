---
title: "2019-10-27 NYC Squirrel Census"
output: html_document:
  code_folding: hide
editor_options: 
  chunk_output_type: console
---

Read in the data
```{r Set Up}
library(tidyverse)
library(tidymodels)
library(lubridate)
library(forcats)
library(knitr)
```


```{r Clean Data}
nyc_squirrels_raw <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-29/nyc_squirrels.csv")
nyc_squirrels <- nyc_squirrels_raw %>% mutate(shift = as_factor(shift),
                                              date = mdy(date),
                                              age = fct_explicit_na(fct_relevel(age,"Juvenile")),
                                              primary_fur_color = fct_explicit_na(primary_fur_color),
                                              #highlight_fur_color = fct_explicit_na(highlight_fur_color))
                                              location = fct_explicit_na(fct_relevel(location,"Ground Plane")))

nyc_squirrels <- nyc_squirrels %>%  select(date,age,primary_fur_color,location,running,chasing,climbing,eating,foraging, kuks,tail_flags,approaches,indifferent,runs_from)

```

This is a great dataset with lots of variables. I'm going to use this dataset to practise my machine learning techniques.

I'm going to create a model that can accuractely predict if a squirrel is on the ground or above the ground.

I'm going to be using the tidymodels workflow, and following this helpful guide:
https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c
https://www.benjaminsorensen.me/post/modeling-with-parsnip-and-tidymodels/

Given that there are only 64 missing data points for location, I will remove these.
```{r}
nyc_squirrels %>% count(location)
nyc_squirrels <- nyc_squirrels %>% filter(location != "(Missing)") %>% droplevels()

levels(nyc_squirrels$location)
```

### Train and Test Split

Firstly, create a randomised training and test plit of the original data.
```{r}
set.seed(seed = 1) 
train_test_split <- initial_split(data = nyc_squirrels,prop = 0.8)

train_set <- training(train_test_split)
test_set <- testing(train_test_split)

```

### Create recipe
Here we prep a recipe for how we want to transform our data and then apply these steps to the train and test set
```{r}
recipe <- recipe(location ~ ., data = train_set) %>% 
  step_dummy(primary_fur_color,age,one_hot = T) %>% 
  prep(train_set)

train_baked <- bake(recipe,new_data = train_set)
test_baked <- bake(recipe,new_data = test_set)
```

### Fit the model
```{r}
boost_tree <- boost_tree(mode = "classification") %>% 
  set_engine("xgboost") %>% 
  fit(location ~ ., data = train_baked)

importance <- xgb.importance(feature_names = colnames(train_baked %>% select(-location)),model = boost_tree$fit)

xgb.plot.importance(importance)
```

### Performance Assesment
```{r}
predictions_xgb <- boost_tree %>% 
  predict(new_data = test_baked) %>% 
  bind_cols(test_baked %>% select(location))
```
There are several metrics that can be used to investigate the performance of a classification model but for simplicity Iâ€™m only focusing on a selection of them: accuracy, precision, recall and F1_Score.

```{r}
predictions_xgb %>%
  conf_mat(location, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  ggplot(aes(Prediction, Truth, alpha = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = "white", alpha = 1, size = 8)
```


```{r}
predictions_xgb %>%
  conf_mat(location, .pred_class) %>% summary() %>% 
  select(-.estimator) %>%
  filter(.metric %in%
    c("accuracy", "precision", "recall", "f_meas")) %>%
  kable(digits = 2)
```

Wow, so on first go it seems we have built a model which can fairly accuractely predict if a squirrel is above ground or not.

### Hyper Parameter Tuning

```{r}
folds <- vfold_cv(train_baked,20)

folded <- folds %>% bind_cols(mtry = 1:20) %>% mutate(
  # First create analysis and assesment sets
  analysis = map(splits, analysis),
  assesment = map(splits, assessment),
  # Run Model against different mtry (number of predictors that will be randomly sampled at each split
  # when creating the tree models)
  boost_tree = map2(
  analysis,
  mtry,
  ~ boost_tree(mode = "classification", mtry = .y, ) %>%
  set_engine("xgboost") %>%
  fit(location ~ ., data = .x)
  ),
  # Create prediction for each model
  predictions_xgb = map2(
  assesment,
  boost_tree,
  ~
  predict(.y, new_data = .x) %>%
  bind_cols(.x %>% select(location))
  ),
  accuracy = map(predictions_xgb,~ accuracy(., truth = location, estimate = .pred_class))
  ) %>% 
  unnest(accuracy)

folded %>% ggplot(aes(x = mtry, y = .estimate)) + geom_line() # Best performance is mtry = 19 then 6 then 1
           
```



