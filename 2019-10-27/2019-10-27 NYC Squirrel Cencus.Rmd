---
title: "2019-10-27 NYC Squirrel Census"
output: rmarkdown::github_document
editor_options: 
  chunk_output_type: console
---
# 2019-10-27 NYC Squirrel Census

This is a great dataset with lots of variables. I'm going to use this dataset to practise my machine learning techniques.

I'm going to create a model that can accuractely predict if a squirrel is on the ground or above the ground.

I'm going to be using the tidymodels workflow, and following this helpful guide:
https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c
https://www.benjaminsorensen.me/post/modeling-with-parsnip-and-tidymodels/
https://www.alexpghayes.com/blog/implementing-the-super-learner-with-tidymodels/

Read in the data
```{r Set Up, message=FALSE, warning=FALSE, paged.print=TRUE}
knitr::opts_chunk$set(
  fig.path = "README_figs/README-"
)
# Load libraries
library(tidyverse)
library(tidymodels)
library(lubridate)
library(forcats)
library(knitr)
library(janitor)
library(xgboost)
```

Now clean the dataset so all the column types are in their correctf format.

```{r Clean Data, warning=FALSE}
nyc_squirrels_raw <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-29/nyc_squirrels.csv")
nyc_squirrels <- nyc_squirrels_raw %>% mutate(shift = as_factor(shift),
                                              date = mdy(date),
                                              age = fct_explicit_na(fct_relevel(age,"Juvenile")),
                                              primary_fur_color = fct_explicit_na(primary_fur_color),
                                              #highlight_fur_color = fct_explicit_na(highlight_fur_color))
                                              location = fct_explicit_na(fct_relevel(location,"Ground Plane")))
# Only interested in the below variables
nyc_squirrels <- nyc_squirrels %>%  select(age,primary_fur_color,location,running,chasing,climbing,eating,foraging, kuks,tail_flags,approaches,indifferent,runs_from)

nyc_squirrels
```

Given that there are only 64 missing data points for `location`, I will remove these. Also, I will remove the 55 missing data points for `primary_fur_colour`.

```{r warning=FALSE}
nyc_squirrels %>% count(location)
nyc_squirrels <- nyc_squirrels %>% filter(location != "(Missing)",primary_fur_color != "(Missing)") %>% droplevels()

```

### Benchmark

First we will set a benchmark score to beat with our model by simply guessing that a squirel will be Above Plane when climbing and Ground plane when not.

```{r warning=FALSE}

model_benchmark <- nyc_squirrels %>% 
  mutate(guess = as_factor(if_else(climbing,'Above Ground','Ground Plane'))) %>% 
  accuracy(estimate = guess,truth = location) %>% select(.estimate) %>% pull()
```

Well, it turns out guessing it's Above Plane if it's climbing gets you a accuracy of `r round(model_benchmark,2)`.

## Model
### Train and Test Split
Firstly, create a randomised training and test split of the original data.

```{r warning=FALSE}
set.seed(seed = 1) 
train_test_split <- initial_split(data = nyc_squirrels,prop = 0.8)

train_set <- training(train_test_split)
test_set <- testing(train_test_split)

```

### Create recipe
Here we prep a recipe for how we want to transform our data and then apply these steps to the train and test set.

```{r warning=FALSE}
set.seed(seed = 1) 
recipe <- recipe(location ~ ., data = train_set) %>% 
  step_dummy(primary_fur_color,age,one_hot = T) %>% 
  prep(train_set)

train_baked <- bake(recipe,new_data = train_set)
test_baked <- bake(recipe,new_data = test_set)
```

### Fit the model

```{r warning=FALSE}
set.seed(seed = 1) 
boost_tree <- boost_tree(mode = "classification") %>% 
  set_engine("xgboost") %>% 
  fit(location ~ ., data = train_baked)

importance <- xgb.importance(feature_names = colnames(train_baked %>% select(-location)),model = boost_tree$fit)

xgb.plot.importance(importance)

```

### Performance Assesment

```{r warning=FALSE}
predictions_xgb <- boost_tree %>% 
  predict(new_data = test_baked) %>% 
  bind_cols(test_baked %>% select(location))
```

There are several metrics that can be used to investigate the performance of a classification model but for simplicity Iâ€™m only focusing on a selection of them: accuracy, precision, recall and F1_Score.

```{r warning=FALSE}
predictions_xgb %>%
  conf_mat(location, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  ggplot(aes(Prediction, Truth, alpha = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = "white", alpha = 1, size = 8)
```

```{r warning=FALSE}
model_metrics <- predictions_xgb %>%
  conf_mat(location, .pred_class) %>% summary() %>% 
  select(-.estimator) %>%
  filter(.metric %in%
    c("accuracy", "precision", "recall", "f_meas"))

model_accuracy <- model_metrics %>% filter(.metric == "accuracy") %>% select(.estimate) %>% pull()
model_metrics %>%
  kable(digits = 2)
```

It seems our model score of `r round(model_accuracy,2)` is worse than the benchmark set.

```{r warning=FALSE}
boost_tree %>% 
  predict(new_data = test_baked,type = "prob") %>% clean_names %>% select(pred_ground_plane) %>% 
  bind_cols(test_baked %>% select(location)) %>% 
  gain_curve(truth = location,pred_ground_plane) %>% autoplot()
  
```

### Hyper Parameter Tuning
Create a random grid of parameters object values. Parameters we can tune can be found here: 
https://tidymodels.github.io/parsnip/reference/boost_tree.html

Guide: 

*Control Model Complexitiy - max_depth, min_child_weight, and gamma 
*Robust to noise - subsample, colsample_bytree


```{r warning=FALSE}
library(dials)
set.seed(seed = 1) 

xgb_grid <- grid_random(
trees(c(1,100)), # Number of boosting iterations
min_n(c(1,20)), # Min number of data points in a node to split it further
tree_depth(c(1,20)), # Number of splits
learn_rate(c(0,1),trans = NULL), # eta
loss_reduction(c(0,1),trans = NULL), # gamma
size = 10) %>% mutate(model_id = row_number()) 

xgb_grid
```

Now use K-Fold Cross-Validation to determine which set of parameters is the most accurate. 

Reminder:
3-Fold CV is split into 3 equal sized groups and is tested like below:
*Model1: Trained on Fold1 + Fold2, Tested on Fold3
*Model2: Trained on Fold2 + Fold3, Tested on Fold1
*Model3: Trained on Fold1 + Fold3, Tested on Fold2

https://machinelearningmastery.com/k-fold-cross-validation/

### Tune 1

```{r warning=FALSE}
set.seed(seed = 1) 
folds <- vfold_cv(train_set,5,2)

set.seed(seed = 1) 
folded <-  folds %>% 
  # Cross join the xvg grid create above
  expand_grid(xgb_grid) %>% 
  # Then create analysis and assesment sets
  mutate(analysis = map(splits, analysis),
  assesment = map(splits, assessment),
  # Create a Recipe. Prepper is a wrapper for `prep()` which handles `split` objects
  recipe = map(splits,~prepper(.x,recipe(location ~ ., data = train_set) %>% 
  step_dummy(primary_fur_color,age,one_hot = T))),
  # Now bake analysis and assesment sets
  analysis = map2(recipe,analysis,bake),
  assesment = map2(recipe,assesment,bake),
  # Run model against analysis set with values for parameter objects defined
  boost_tree = pmap(list(trees,min_n,tree_depth,learn_rate,loss_reduction,analysis),
  ~ boost_tree(mode = "classification",trees= ..1,min_n = ..2,tree_depth= ..3,learn_rate = ..4, loss_reduction =  ..5) %>% 
  set_engine("xgboost") %>%
  fit(location ~ ., data = ..6)
  ),
  # Create predictions for each model on the assesment set
  predictions_xgb = map2(
  assesment,
  boost_tree,
  ~
  predict(.y, new_data = .x) %>%
  bind_cols(.x %>% select(location))
  ),
  # Compute accuracy metric
  accuracy = map(predictions_xgb,~ accuracy(., truth = location, estimate = .pred_class))
  ) %>% 
  unnest(accuracy)



# Visualise model performance by model id
folded %>% ggplot(aes(x = as_factor(model_id), y = .estimate)) + geom_boxplot()

# Table of top model combinations
hi <- folded %>% group_by(model_id) %>% summarise(.estimate = median(.estimate)) %>% ungroup( )%>% filter(.estimate >model_benchmark) %>% 
  inner_join(xgb_grid) %>% arrange(desc(.estimate))
hi
#  Variance of model accuracy by paramter values
folded %>% select(trees:model_id,.estimate) %>% group_by(model_id)%>% pivot_longer(trees:loss_reduction) %>% ggplot(aes(x = value,y = .estimate,colour = as_factor(model_id))) + geom_point() + facet_wrap(~name,scales ="free_x") 

# Mean accuracy for each parameter values
folded %>% select(trees:model_id,.estimate) %>% group_by(model_id)%>% pivot_longer(trees:loss_reduction) %>% group_by(name,value) %>% summarise(.estimate = mean(.estimate)) %>% ggplot(aes(x = value,y = .estimate)) + geom_point() + facet_wrap(~name,scales ="free_x") 
             
```

Looking at the graph which shows mean accuracy by parameter. We can see the following:

*`min_n` performs best when roughly 10.
*`trees` perfroms best when greater than 50
*`loss_reduction` performs better when greater than 0.25

The other paramters show no clear pattern so will only change the values for the above.

# Tune 2
```{r warning=FALSE}
set.seed(seed = 2) 
xgb_grid <- grid_random(
trees(c(50,100)), # Number of boosting iterations
min_n(c(7,13)), # Min number of data points in a node to split it further
tree_depth(c(1,20)), # Number of splits
learn_rate(c(0,1),trans = NULL), # eta
loss_reduction(c(0.3,1),trans = NULL), # gamma
size = 10) %>% mutate(model_id = row_number()) 

xgb_grid
```

```{r warning=FALSE}
set.seed(seed = 2) 
folds <- vfold_cv(train_set,5,2)

folded <-  folds %>% 
  # Cross join the xvg grid create above
  expand_grid(xgb_grid) %>% 
  # Then create analysis and assesment sets
  mutate(analysis = map(splits, analysis),
  assesment = map(splits, assessment),
  # Create a Recipe. Prepper is a wrapper for `prep()` which handles `split` objects
  recipe = map(splits,~prepper(.x,recipe(location ~ ., data = train_set) %>% 
  step_dummy(primary_fur_color,age,one_hot = T))),
  # Now bake analysis and assesment sets
  analysis = map2(recipe,analysis,bake),
  assesment = map2(recipe,assesment,bake),
  # Run model against analysis set with values for parameter objects defined
  boost_tree = pmap(list(trees,min_n,tree_depth,learn_rate,loss_reduction,analysis),
  ~ boost_tree(mode = "classification",trees= ..1,min_n = ..2,tree_depth= ..3,learn_rate = ..4, loss_reduction =  ..5) %>% 
  set_engine("xgboost") %>%
  fit(location ~ ., data = ..6)
  ),
  # Create predictions for each model on the assesment set
  predictions_xgb = map2(
  assesment,
  boost_tree,
  ~
  predict(.y, new_data = .x) %>%
  bind_cols(.x %>% select(location))
  ),
  # Compute accuracy metric
  accuracy = map(predictions_xgb,~ accuracy(., truth = location, estimate = .pred_class))
  ) %>% 
  unnest(accuracy)



# Visualise model performance by model id
folded %>% ggplot(aes(x = as_factor(model_id), y = .estimate)) + geom_boxplot()

# Table of top model combinations
hi <- folded %>% group_by(model_id) %>% summarise(.estimate = median(.estimate)) %>% ungroup( )%>% filter(.estimate >model_benchmark) %>% 
  inner_join(xgb_grid) %>% arrange(desc(.estimate))
hi
#  Variance of model accuracy by paramter values
folded %>% select(trees:model_id,.estimate) %>% group_by(model_id)%>% pivot_longer(trees:loss_reduction) %>% ggplot(aes(x = value,y = .estimate,colour = as_factor(model_id))) + geom_point() + facet_wrap(~name,scales ="free_x") 

# Mean accuracy for each parameter values
folded %>% select(trees:model_id,.estimate) %>% group_by(model_id)%>% pivot_longer(trees:loss_reduction) %>% group_by(name,value) %>% summarise(.estimate = mean(.estimate)) %>% ggplot(aes(x = value,y = .estimate)) + geom_point() + facet_wrap(~name,scales ="free_x") 
             
```

Looking at the graph which shows mean accuracy by parameter. We can see the following:

*`min_n` performs best between 7 and 9.
*`tree_depth` perfroms best between greater than 4 and 9
*`loss_reduction` performs better when less than 0.9

The other paramters show no clear pattern so will only change the values for the above.

### Tune 3

```{r warning=FALSE}
set.seed(seed = 3) 
xgb_grid <- grid_random(
trees(c(50,100)), # Number of boosting iterations
min_n(c(7,9)), # Min number of data points in a node to split it further
tree_depth(c(4,9)), # Number of splits
learn_rate(c(0,1),trans = NULL), # eta
loss_reduction(c(0.3,0.9),trans = NULL), # gamma
size = 10) %>% mutate(model_id = row_number()) 

xgb_grid
```

```{r warning=FALSE}
set.seed(seed = 3) 
folds <- vfold_cv(train_set,5,2)

folded <-  folds %>% 
  # Cross join the xvg grid create above
  expand_grid(xgb_grid) %>% 
  # Then create analysis and assesment sets
  mutate(analysis = map(splits, analysis),
  assesment = map(splits, assessment),
  # Create a Recipe. Prepper is a wrapper for `prep()` which handles `split` objects
  recipe = map(splits,~prepper(.x,recipe(location ~ ., data = train_set) %>% 
  step_dummy(primary_fur_color,age,one_hot = T))),
  # Now bake analysis and assesment sets
  analysis = map2(recipe,analysis,bake),
  assesment = map2(recipe,assesment,bake),
  # Run model against analysis set with values for parameter objects defined
  boost_tree = pmap(list(trees,min_n,tree_depth,learn_rate,loss_reduction,analysis),
  ~ boost_tree(mode = "classification",trees= ..1,min_n = ..2,tree_depth= ..3,learn_rate = ..4, loss_reduction =  ..5) %>% 
  set_engine("xgboost") %>%
  fit(location ~ ., data = ..6)
  ),
  # Create predictions for each model on the assesment set
  predictions_xgb = map2(
  assesment,
  boost_tree,
  ~
  predict(.y, new_data = .x) %>%
  bind_cols(.x %>% select(location))
  ),
  # Compute accuracy metric
  accuracy = map(predictions_xgb,~ accuracy(., truth = location, estimate = .pred_class))
  ) %>% 
  unnest(accuracy)



# Visualise model performance by model id
folded %>% ggplot(aes(x = as_factor(model_id), y = .estimate)) + geom_boxplot()

# Table of top model combinations
hi <- folded %>% group_by(model_id) %>% summarise(.estimate = median(.estimate)) %>% ungroup( )%>% filter(.estimate >model_benchmark) %>% 
  inner_join(xgb_grid) %>% arrange(desc(.estimate))
hi
#  Variance of model accuracy by paramter values
folded %>% select(trees:model_id,.estimate) %>% group_by(model_id)%>% pivot_longer(trees:loss_reduction) %>% ggplot(aes(x = value,y = .estimate,colour = as_factor(model_id))) + geom_point() + facet_wrap(~name,scales ="free_x") 

# Mean accuracy for each parameter values
folded %>% select(trees:model_id,.estimate) %>% group_by(model_id)%>% pivot_longer(trees:loss_reduction) %>% group_by(name,value) %>% summarise(.estimate = mean(.estimate)) %>% ggplot(aes(x = value,y = .estimate)) + geom_point() + facet_wrap(~name,scales ="free_x") 
             
```

## Choose parameters
Our best performing model was using the following parameters:

Now let's use this on the testing set.

### Fit the model
```{r warning=FALSE}
set.seed(seed = 1) 
boost_tree <-
  boost_tree(
  mode = "classification",
  trees = 89,
  min_n = 7,
  tree_depth = 7,
  learn_rate = 0.314,
  loss_reduction = 0.377
  ) %>%
  set_engine("xgboost") %>%
  fit(location ~ ., data = train_baked)

importance <- xgb.importance(feature_names = colnames(train_baked %>% select(-location)),model = boost_tree$fit)

xgb.plot.importance(importance)
```

### Performance Assesment
```{r warning=FALSE}
predictions_xgb <- boost_tree %>% 
  predict(new_data = test_baked) %>% 
  bind_cols(test_baked %>% select(location))
```

```{r warning=FALSE}
predictions_xgb %>%
  conf_mat(location, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  ggplot(aes(Prediction, Truth, alpha = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = "white", alpha = 1, size = 8)
```

```{r warning=FALSE}
predictions_xgb %>%
  conf_mat(location, .pred_class) %>% summary() %>% 
  select(-.estimator) %>%
  filter(.metric %in%
    c("accuracy", "precision", "recall", "f_meas")) %>%
  kable(digits = 2)
```


### Next Steps: Fit the metalearner
Use a multinomial regression as the metalearner.



